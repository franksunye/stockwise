"""
LLM å®¢æˆ·ç«¯æ¨¡å—
å°è£…æœ¬åœ° LLM ä»£ç†æœåŠ¡çš„è°ƒç”¨é€»è¾‘
"""

import json
import requests
from typing import Optional, Dict, Any, Tuple
import time
try:
    from backend.config import LLM_CONFIG
except ImportError:
    from config import LLM_CONFIG
from .llm_tracker import get_tracker, estimate_tokens
from .schema_normalizer import normalize_ai_response

import asyncio

class AsyncRateLimiter:
    """ç®€å•çš„å¼‚æ­¥é€Ÿç‡é™åˆ¶å™¨ (Token Bucket æ€æƒ³)"""
    def __init__(self, rate: float):
        self._interval = 1.0 / rate if rate > 0 else 0
        self._last_check = 0.0
        self._lock = asyncio.Lock()

    async def acquire(self):
        if self._interval <= 0: return
        async with self._lock:
            now = time.time()
            elapsed = now - self._last_check
            wait_time = self._interval - elapsed
            if wait_time > 0:
                await asyncio.sleep(wait_time)
            self._last_check = time.time() + (wait_time if wait_time > 0 else 0)

class LLMClient:
    """æœ¬åœ° LLM ä»£ç†å®¢æˆ·ç«¯"""
    
    # å…¨å±€å…±äº«çš„é™æµå™¨ (Provider -> AsyncRateLimiter)
    _rate_limiters = {}
    
    def __init__(
        self,
        provider: str = None,
        base_url: str = None,
        api_key: str = None,
        model: str = None,
        timeout: int = 120
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        """
        self.provider = provider or LLM_CONFIG.get("provider", "openai")
        self.timeout = timeout
        
        # è‡ªåŠ¨æ³¨å†Œ Hunyuan é™æµå™¨
        if self.provider == "hunyuan" and "hunyuan" not in self._rate_limiters:
            qps = LLM_CONFIG.get("hunyuan", {}).get("qps_limit", 2.0)
            self._rate_limiters["hunyuan"] = AsyncRateLimiter(qps)
        
        # æ ¹æ®æä¾›å•†åŠ è½½é»˜è®¤é…ç½®
        if self.provider == "deepseek":
            ds_config = LLM_CONFIG.get("deepseek", {})
            self.base_url = base_url or ds_config.get("base_url") or "https://api.deepseek.com/v1"
            self.api_key = api_key or ds_config.get("api_key") or LLM_CONFIG.get("api_key")
            self.model = model or ds_config.get("model") or "deepseek-chat"
        elif self.provider == "gemini":
            gm_config = LLM_CONFIG.get("gemini", {})
            self.api_key = api_key or gm_config.get("api_key") or LLM_CONFIG.get("api_key")
            self.model = model or gm_config.get("model") or "gemini-pro"
            self.base_url = base_url # Gemini native usually doesn't use base_url in standard requests
        elif self.provider == "gemini_local":
            # æ–°å¢: é€šè¿‡ Gemini SDK è¿æ¥æœ¬åœ°ä»£ç† (Antigravity Tools)
            gm_local_config = LLM_CONFIG.get("gemini_local", {})
            self.base_url = base_url or gm_local_config.get("base_url") or "http://127.0.0.1:8045"
            self.api_key = api_key or gm_local_config.get("api_key") or LLM_CONFIG.get("api_key")
            self.model = model or gm_local_config.get("model") or "gemini-3-flash"
        elif self.provider == "hunyuan":
            hy_config = LLM_CONFIG.get("hunyuan", {})
            self.base_url = base_url or hy_config.get("base_url") or "https://api.hunyuan.cloud.tencent.com/v1"
            self.api_key = api_key or hy_config.get("api_key")
            self.model = model or hy_config.get("model") or "hunyuan-lite"
        else: # openai, custom, or generic
            self.base_url = base_url or LLM_CONFIG.get("base_url", "http://127.0.0.1:8045/v1")
            self.api_key = api_key or LLM_CONFIG.get("api_key", "")
            self.model = model or LLM_CONFIG.get("model", "gpt-3.5-turbo")

        self.timeout = timeout
        
        # Gemini Native Client ç¼“å­˜ (ç”¨äºäº‘ç«¯ Gemini)
        self._gemini_client = None
        if self.provider == "gemini" and self.api_key:
            try:
                from google import genai
                self._gemini_client = genai.Client(api_key=self.api_key)
            except Exception as e:
                print(f"âš ï¸ åˆå§‹åŒ– Gemini V2 SDK å¤±è´¥: {e}")
        
        # Gemini Local Client ç¼“å­˜ (ç”¨äºæœ¬åœ°ä»£ç†)
        self._gemini_local_client = None
        if self.provider == "gemini_local" and self.api_key:
            try:
                from google import genai
                # V2 SDK support custom endpoint via http_options
                self._gemini_local_client = genai.Client(
                    api_key=self.api_key,
                    http_options={'base_url': self.base_url}
                )
                print(f"âœ… Gemini Local V2 SDK åˆå§‹åŒ–æˆåŠŸ -> {self.base_url}")
            except Exception as e:
                print(f"âš ï¸ åˆå§‹åŒ– Gemini Local V2 SDK å¤±è´¥: {e}")
        
    def is_available(self) -> bool:
        """æ£€æŸ¥ LLM æœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(
                f"{self.base_url}/models",
                timeout=5,
                headers={"Authorization": f"Bearer {self.api_key}"}
            )
            return response.status_code == 200
        except:
            return False
    
    def chat(
        self,
        messages: list,
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ) -> Tuple[Optional[str], Dict[str, Any]]:
        """å‘é€èŠå¤©è¯·æ±‚"""
        if self.provider == "gemini" and self._gemini_client:
            return self._chat_gemini(messages, temperature, max_tokens)
        elif self.provider == "gemini_local" and self._gemini_local_client:
            return self._chat_gemini_local(messages, temperature, max_tokens)
        
        return self._chat_openai_compatible(messages, model, temperature, max_tokens)

    async def chat_async(
        self,
        messages: list,
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ) -> Tuple[Optional[str], Dict[str, Any]]:
        """Async wrapper for chat (using executor)"""
        # Rate Limiting Check
        if self.provider in self._rate_limiters:
            await self._rate_limiters[self.provider].acquire()

        import asyncio
        loop = asyncio.get_running_loop()
        return await loop.run_in_executor(
            None, 
            lambda: self.chat(messages, model, temperature, max_tokens)
        )

    def _chat_openai_compatible(
        self,
        messages: list,
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ) -> Tuple[Optional[str], Dict[str, Any]]:
        payload = {
            "model": model or self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": max_tokens,
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        meta = {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0, "latency_ms": 0, "error": None}
        
        try:
            start_time = time.time()
            response = requests.post(f"{self.base_url}/chat/completions", headers=headers, json=payload, timeout=self.timeout)
            elapsed = time.time() - start_time
            meta["latency_ms"] = int(elapsed * 1000)
            
            if response.status_code == 200:
                data = response.json()
                usage = data.get('usage', {})
                if usage:
                    meta["input_tokens"] = usage.get('prompt_tokens', 0)
                    meta["output_tokens"] = usage.get('completion_tokens', 0)
                    meta["total_tokens"] = usage.get('total_tokens', 0)
                
                if data.get('choices'):
                    content = data['choices'][0].get('message', {}).get('content')
                    if not meta["input_tokens"]:
                        input_text = " ".join([m.get('content', '') for m in messages])
                        meta["input_tokens"] = estimate_tokens(input_text)
                    if not meta["output_tokens"] and content:
                        meta["output_tokens"] = estimate_tokens(content)
                    if not meta["total_tokens"]:
                        meta["total_tokens"] = meta["input_tokens"] + meta["output_tokens"]
                    
                    print(f"   ğŸ¤– {self.provider.upper()} å“åº”æˆåŠŸ ({elapsed:.1f}s, {meta['total_tokens']} tokens)")
                    return content, meta
                else:
                    meta["error"] = f"å“åº”æ ¼å¼å¼‚å¸¸: {data}"
                    return None, meta
            else:
                meta["error"] = f"HTTP {response.status_code}: {response.text[:200]}"
                print(f"   âŒ {self.provider.upper()} è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return None, meta
        except Exception as e:
            meta["error"] = str(e)
            print(f"   âŒ {self.provider.upper()} è¯·æ±‚å¼‚å¸¸: {e}")
            return None, meta

    def _chat_gemini(
        self, 
        messages: list, 
        temperature: float = 0.7, 
        max_tokens: int = 4096
    ) -> Tuple[Optional[str], Dict[str, Any]]:
        meta = {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0, "latency_ms": 0, "error": None}
        try:
            # æå– system åŠ history
            system_msg = ""
            history = []
            
            # æ ¼å¼è½¬æ¢ï¼šRole å¿…é¡»æ˜¯ 'user' æˆ– 'model'
            # System message é€šè¿‡ config ä¼ é€’
            for m in messages:
                if m["role"] == "system":
                    system_msg = m["content"]
                elif m["role"] == "user":
                    history.append({"role": "user", "parts": [{"text": m["content"]}]})
                elif m["role"] == "assistant":
                    history.append({"role": "model", "parts": [{"text": m["content"]}]})
            
            client = self._gemini_client
            start_time = time.time()
            
            # ä½¿ç”¨ V2 SDK è°ƒç”¨
            # æ³¨æ„: V2 SDK çš„ Chat æ¥å£ç•¥æœ‰ä¸åŒï¼Œè¿™é‡Œä½¿ç”¨ models.generate_content é…åˆ history å®ç°å•æ¬¡è°ƒç”¨
            # æˆ–è€…ä½¿ç”¨ chats.create
            
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬ä½¿ç”¨ generate_content (Stateless)
            # éœ€è¦æŠŠ history æ„é€ ä¸º contents
            # æœ€åä¸€ä¸ªä½œä¸º prompt? No, generate_content æ¥å—å®Œæ•´åˆ—è¡¨
            
            contents = history # V2 contents format: list of Content or dict
            
            from google import genai
            from google.genai import types
            
            config = types.GenerateContentConfig(
                temperature=temperature,
                max_output_tokens=max_tokens,
                system_instruction=system_msg if system_msg else None
            )
            
            response = client.models.generate_content(
                model=self.model,
                contents=contents,
                config=config
            )
            
            elapsed = time.time() - start_time
            meta["latency_ms"] = int(elapsed * 1000)
            
            content = response.text
            
            # æå– Token ä½¿ç”¨æƒ…å†µ
            if response.usage_metadata:
                meta["input_tokens"] = response.usage_metadata.prompt_token_count
                meta["output_tokens"] = response.usage_metadata.candidates_token_count
                meta["total_tokens"] = response.usage_metadata.total_token_count
            
            print(f"   ğŸ¤– GEMINI å“åº”æˆåŠŸ ({elapsed:.1f}s, {meta['total_tokens']} tokens)")
            return content, meta
        except Exception as e:
            meta["error"] = str(e)
            print(f"   âŒ GEMINI è¯·æ±‚å¼‚å¸¸: {e}")
            return None, meta
    
    def _chat_gemini_local(
        self, 
        messages: list, 
        temperature: float = 0.7, 
        max_tokens: int = 4096
    ) -> Tuple[Optional[str], Dict[str, Any]]:
        """
        é€šè¿‡æœ¬åœ°ä»£ç†è°ƒç”¨ Gemini V2 SDK
        """
        meta = {"input_tokens": 0, "output_tokens": 0, "total_tokens": 0, "latency_ms": 0, "error": None}
        try:
            # æ„é€ å†…å®¹
            # æ³¨æ„ï¼šå¦‚æœæœ¬åœ°ä»£ç†è¿˜ä¸æ”¯æŒ system_instruction, éœ€è¦æ‰‹åŠ¨åˆå¹¶
            system_msg = ""
            contents = []
            
            for m in messages:
                role = "user"
                if m["role"] == "assistant": role = "model"
                elif m["role"] == "system": 
                    system_msg = m["content"]
                    continue # merge later
                
                contents.append({"role": role, "parts": [{"text": m["content"]}]})
            
            # æ‰‹åŠ¨åˆå¹¶ System Prompt åˆ°ç¬¬ä¸€ä¸ª User Message
            if system_msg and contents:
                 first_part = contents[0]["parts"][0]["text"]
                 contents[0]["parts"][0]["text"] = f"[ç³»ç»ŸæŒ‡ä»¤] {system_msg}\n\n[ç”¨æˆ·æ¶ˆæ¯] {first_part}"
            
            client = self._gemini_local_client
            start_time = time.time()
            
            from google import genai
            from google.genai import types
            
            config = types.GenerateContentConfig(
                temperature=temperature,
                max_output_tokens=max_tokens
            )
            
            response = client.models.generate_content(
                model=self.model,
                contents=contents,
                config=config
            )
            
            elapsed = time.time() - start_time
            meta["latency_ms"] = int(elapsed * 1000)
            
            content = response.text
             
            # Token Usage
            if response.usage_metadata:
                meta["input_tokens"] = response.usage_metadata.prompt_token_count
                meta["output_tokens"] = response.usage_metadata.candidates_token_count
                meta["total_tokens"] = response.usage_metadata.total_token_count
            else:
                meta["input_tokens"] = estimate_tokens(str(messages))
                meta["output_tokens"] = estimate_tokens(content)
                meta["total_tokens"] = meta["input_tokens"] + meta["output_tokens"]
                
            print(f"   ğŸ¤– GEMINI_LOCAL å“åº”æˆåŠŸ ({elapsed:.1f}s, {meta['total_tokens']} tokens)")
            return content, meta
        except Exception as e:
            meta["error"] = str(e)
            print(f"   âŒ GEMINI_LOCAL è¯·æ±‚å¼‚å¸¸: {e}")
            return None, meta
    
    def generate_stock_prediction(
        self,
        system_prompt: str,
        user_prompt: str,
        symbol: str = None,
        retries: int = 2
    ) -> Optional[Dict[str, Any]]:
        """
        ç”Ÿæˆè‚¡ç¥¨é¢„æµ‹ï¼ˆå¸¦ JSON è§£æã€é‡è¯•å’Œè¿½è¸ªï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰è¾“å‡ºæ ¼å¼ï¼‰
            user_prompt: ç”¨æˆ·è¾“å…¥ï¼ˆè‚¡ç¥¨æ•°æ®ï¼‰
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºè¿½è¸ªï¼‰
            retries: é‡è¯•æ¬¡æ•°
            
        Returns:
            è§£æåçš„é¢„æµ‹ç»“æœ dictï¼Œå¤±è´¥è¿”å› None
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # å¼€å§‹è¿½è¸ª
        tracker = get_tracker()
        tracker.start_trace(symbol=symbol, model=self.model)
        tracker.set_prompts(system_prompt, user_prompt)
        
        final_content = None
        final_result = None
        last_meta = {}
        
        for attempt in range(retries + 1):
            if attempt > 0:
                print(f"   ğŸ”„ é‡è¯• {attempt}/{retries}...")
                tracker.increment_retry()
                
            content, meta = self.chat(messages, temperature=0.5)
            last_meta = meta
            
            if content:
                final_content = content
                # å°è¯•è§£æ JSON
                result = self._parse_json_response(content)
                if result:
                    # æ ‡å‡†åŒ–æ•°æ®ç»“æ„ / Normalize schema
                    result = normalize_ai_response(result)
                    final_result = result
                    break
                else:
                    print(f"   âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹:\n{content[:500]}...")
        
        # è®°å½•è¿½è¸ªç»“æœ
        tracker.set_tokens(
            input_tokens=last_meta.get("input_tokens", 0),
            output_tokens=last_meta.get("output_tokens", 0),
            total_tokens=last_meta.get("total_tokens", 0)
        )
        tracker.set_response(final_content, final_result)
        
        if final_result:
            tracker.set_status("success")
        elif final_content:
            tracker.set_status("parse_failed", "JSON è§£æå¤±è´¥")
        else:
            tracker.set_status("error", last_meta.get("error", "æœªçŸ¥é”™è¯¯"))
        
        # ç»“æŸè¿½è¸ªå¹¶ä¿å­˜
        trace = tracker.end_trace()
        if trace:
            status_emoji = "âœ…" if trace.status == "success" else "âŒ"
            print(f"   ğŸ“Š è¿½è¸ªå®Œæˆ: {status_emoji} {trace.latency_ms}ms | {trace.total_tokens} tokens | é‡è¯• {trace.retry_count} æ¬¡")
        
        return final_result
    
    def _parse_json_response(self, content: str) -> Optional[Dict[str, Any]]:
        """
        è§£æ LLM è¿”å›çš„ JSON å†…å®¹ï¼ˆæ·±åº¦æ¸…æ´—ç‰ˆï¼‰
        é’ˆå¯¹: Markdown å—ã€éæ ‡å‡†å¼•å·ã€å¼€å¤´ç»“å°¾ä¹±ç ã€è‡ªåŠ¨æˆªæ–­ä¿®å¤
        """
        if not content:
            return None
        
        # 1. å°è¯•æ ‡å‡†è§£æ
        try:
            return json.loads(content)
        except:
            pass
            
        import re
        
        # 2. ç§»é™¤å¸¸è§çš„ Markdown æ ‡è®°
        content_clean = re.sub(r'^```json\s*', '', content, flags=re.MULTILINE)
        content_clean = re.sub(r'^```\s*', '', content_clean, flags=re.MULTILINE)
        content_clean = re.sub(r'```$', '', content_clean, flags=re.MULTILINE)
        try:
            return json.loads(content_clean)
        except:
            pass
            
        # 3. æš´åŠ›æå–æœ€å¤–å±‚çš„ {}
        try:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª {
            start_idx = content.find('{')
            if start_idx != -1:
                # å€’åºæ‰¾åˆ°æœ€åä¸€ä¸ª }
                end_idx = content.rfind('}')
                if end_idx != -1 and end_idx > start_idx:
                    possible_json = content[start_idx : end_idx + 1]
                    # å°è¯•æ¸…ç†å¯èƒ½æ··å…¥çš„æ¢è¡Œç¬¦é—®é¢˜
                    possible_json = re.sub(r',\s*}', '}', possible_json) # ç§»é™¤å°¾éšé€—å·
                    return json.loads(possible_json)
        except:
            pass

        # 4. å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œå°è¯•ä½¿ç”¨æ ˆå¹³è¡¡æ³•æ‰¾åˆ°å®Œæ•´çš„å¯¹è±¡ (é’ˆå¯¹ç²˜åŒ…/æˆªæ–­)
        try:
            balance = 0
            start = content.find('{')
            if start != -1:
                for i in range(start, len(content)):
                    char = content[i]
                    if char == '{':
                        balance += 1
                    elif char == '}':
                        balance -= 1
                        if balance == 0:
                            # æ‰¾åˆ°äº†ä¸€ä¸ªå®Œæ•´çš„é¡¶å±‚å¯¹è±¡
                            return json.loads(content[start:i+1])
        except:
            pass
            
        return None


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """è·å–å…¨å±€ LLM å®¢æˆ·ç«¯å®ä¾‹"""
    global _client
    if _client is None:
        _client = LLMClient()
    return _client


def test_llm_connection() -> bool:
    """æµ‹è¯• LLM è¿æ¥"""
    client = get_llm_client()
    if not client.is_available():
        print("âŒ LLM æœåŠ¡ä¸å¯ç”¨")
        return False
    
    print("âœ… LLM æœåŠ¡è¿æ¥æˆåŠŸ")
    response, meta = client.chat([{"role": "user", "content": "å›å¤'OK'"}])
    if response:
        print(f"   æµ‹è¯•å“åº”: {response[:50]}...")
        print(f"   Token ä½¿ç”¨: {meta.get('total_tokens', 'N/A')}")
        return True
    return False


if __name__ == "__main__":
    test_llm_connection()
