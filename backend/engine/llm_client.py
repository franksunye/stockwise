"""
LLM å®¢æˆ·ç«¯æ¨¡å—
å°è£…æœ¬åœ° LLM ä»£ç†æœåŠ¡çš„è°ƒç”¨é€»è¾‘
"""

import json
import requests
from typing import Optional, Dict, Any, Tuple
import time
from config import LLM_CONFIG
from .llm_tracker import get_tracker, estimate_tokens


class LLMClient:
    """æœ¬åœ° LLM ä»£ç†å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        base_url: str = None,
        api_key: str = None,
        model: str = None,
        timeout: int = 120
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        
        Args:
            base_url: API åŸºç¡€åœ°å€ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®
            api_key: API å¯†é’¥ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.base_url = base_url or LLM_CONFIG.get("base_url", "http://127.0.0.1:8045/v1")
        self.api_key = api_key or LLM_CONFIG.get("api_key", "")
        self.model = model or LLM_CONFIG.get("model", "gpt-3.5-turbo")
        self.timeout = timeout
        
    def is_available(self) -> bool:
        """æ£€æŸ¥ LLM æœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(
                f"{self.base_url}/models",
                timeout=5,
                headers={"Authorization": f"Bearer {self.api_key}"}
            )
            return response.status_code == 200
        except:
            return False
    
    def chat(
        self,
        messages: list,
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 2000
    ) -> Tuple[Optional[str], Dict[str, Any]]:
        """
        å‘é€èŠå¤©è¯·æ±‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ [{"role": "user/system/assistant", "content": "..."}]
            model: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¯è¦†ç›–é»˜è®¤ï¼‰
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§è¾“å‡º token æ•°
            
        Returns:
            Tuple: (LLM è¿”å›çš„æ–‡æœ¬å†…å®¹, å…ƒæ•°æ® dict)
            å…ƒæ•°æ®åŒ…å«: input_tokens, output_tokens, total_tokens, latency_ms, error
        """
        payload = {
            "model": model or self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": 4000, # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é•¿åº¦ç”Ÿæˆå®Œæ•´çš„ JSON
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        meta = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "latency_ms": 0,
            "error": None
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=self.timeout
            )
            elapsed = time.time() - start_time
            meta["latency_ms"] = int(elapsed * 1000)
            
            if response.status_code == 200:
                data = response.json()
                
                # æå– Token ä½¿ç”¨é‡ (å¦‚æœ API è¿”å›)
                usage = data.get('usage', {})
                if usage:
                    meta["input_tokens"] = usage.get('prompt_tokens', 0)
                    meta["output_tokens"] = usage.get('completion_tokens', 0)
                    meta["total_tokens"] = usage.get('total_tokens', 0)
                
                if data.get('choices'):
                    content = data['choices'][0].get('message', {}).get('content')
                    
                    # å¦‚æœ API æ²¡æœ‰è¿”å› token æ•°ï¼Œä½¿ç”¨ä¼°ç®—
                    if not meta["input_tokens"]:
                        input_text = " ".join([m.get('content', '') for m in messages])
                        meta["input_tokens"] = estimate_tokens(input_text)
                    if not meta["output_tokens"] and content:
                        meta["output_tokens"] = estimate_tokens(content)
                    if not meta["total_tokens"]:
                        meta["total_tokens"] = meta["input_tokens"] + meta["output_tokens"]
                    
                    print(f"   ğŸ¤– LLM å“åº”æˆåŠŸ ({elapsed:.1f}s, {meta['total_tokens']} tokens)")
                    return content, meta
                else:
                    meta["error"] = f"å“åº”æ ¼å¼å¼‚å¸¸: {data}"
                    print(f"   âš ï¸ LLM å“åº”æ ¼å¼å¼‚å¸¸: {data}")
                    return None, meta
            else:
                meta["error"] = f"HTTP {response.status_code}"
                print(f"   âŒ LLM è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return None, meta
                
        except requests.exceptions.Timeout:
            meta["error"] = f"è¯·æ±‚è¶…æ—¶ ({self.timeout}s)"
            print(f"   âŒ LLM è¯·æ±‚è¶…æ—¶ ({self.timeout}s)")
            return None, meta
        except requests.exceptions.ConnectionError:
            meta["error"] = f"æ— æ³•è¿æ¥ LLM æœåŠ¡: {self.base_url}"
            print(f"   âŒ æ— æ³•è¿æ¥ LLM æœåŠ¡: {self.base_url}")
            return None, meta
        except Exception as e:
            meta["error"] = str(e)
            print(f"   âŒ LLM è¯·æ±‚å¼‚å¸¸: {e}")
            return None, meta
    
    def generate_stock_prediction(
        self,
        system_prompt: str,
        user_prompt: str,
        symbol: str = None,
        retries: int = 2
    ) -> Optional[Dict[str, Any]]:
        """
        ç”Ÿæˆè‚¡ç¥¨é¢„æµ‹ï¼ˆå¸¦ JSON è§£æã€é‡è¯•å’Œè¿½è¸ªï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰è¾“å‡ºæ ¼å¼ï¼‰
            user_prompt: ç”¨æˆ·è¾“å…¥ï¼ˆè‚¡ç¥¨æ•°æ®ï¼‰
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºè¿½è¸ªï¼‰
            retries: é‡è¯•æ¬¡æ•°
            
        Returns:
            è§£æåçš„é¢„æµ‹ç»“æœ dictï¼Œå¤±è´¥è¿”å› None
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # å¼€å§‹è¿½è¸ª
        tracker = get_tracker()
        tracker.start_trace(symbol=symbol, model=self.model)
        tracker.set_prompts(system_prompt, user_prompt)
        
        final_content = None
        final_result = None
        last_meta = {}
        
        for attempt in range(retries + 1):
            if attempt > 0:
                print(f"   ğŸ”„ é‡è¯• {attempt}/{retries}...")
                tracker.increment_retry()
                
            content, meta = self.chat(messages, temperature=0.5)
            last_meta = meta
            
            if content:
                final_content = content
                # å°è¯•è§£æ JSON
                result = self._parse_json_response(content)
                if result:
                    final_result = result
                    break
                else:
                    print(f"   âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹:\n{content[:500]}...")
        
        # è®°å½•è¿½è¸ªç»“æœ
        tracker.set_tokens(
            input_tokens=last_meta.get("input_tokens", 0),
            output_tokens=last_meta.get("output_tokens", 0),
            total_tokens=last_meta.get("total_tokens", 0)
        )
        tracker.set_response(final_content, final_result)
        
        if final_result:
            tracker.set_status("success")
        elif final_content:
            tracker.set_status("parse_failed", "JSON è§£æå¤±è´¥")
        else:
            tracker.set_status("error", last_meta.get("error", "æœªçŸ¥é”™è¯¯"))
        
        # ç»“æŸè¿½è¸ªå¹¶ä¿å­˜
        trace = tracker.end_trace()
        if trace:
            status_emoji = "âœ…" if trace.status == "success" else "âŒ"
            print(f"   ğŸ“Š è¿½è¸ªå®Œæˆ: {status_emoji} {trace.latency_ms}ms | {trace.total_tokens} tokens | é‡è¯• {trace.retry_count} æ¬¡")
        
        return final_result
    
    def _parse_json_response(self, content: str) -> Optional[Dict[str, Any]]:
        """
        è§£æ LLM è¿”å›çš„ JSON å†…å®¹ï¼ˆå¤„ç† markdown ä»£ç å—ã€åµŒå¥—åŠä¸å®Œæ•´å†…å®¹ï¼‰
        """
        if not content:
            return None
            
        # æ¸…ç†å¸¸è§çš„å¹²æ‰°å­—ç¬¦
        content = content.strip()
        
        # 1. å°è¯•ç›´æ¥è§£æ
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            pass
            
        import re
        
        # 2. å°è¯•æå–å®Œæ•´é—­åˆçš„ ```json ... ``` å—ï¼ˆéè´ªå©ªåŒ¹é…ï¼‰
        # ä¼˜å…ˆå°è¯•æœ€å‰é¢çš„å—ï¼Œå› ä¸ºåé¢çš„å—å¯èƒ½æ˜¯é‡å¤ä¸”æˆªæ–­çš„
        json_blocks = re.findall(r'```json\s*(.*?)\s*```', content, re.DOTALL)
        
        for block in json_blocks:
            try:
                # å†æ¬¡æ¸…ç†å—å†…å¯èƒ½å­˜åœ¨çš„åµŒå¥—å¹»è§‰ï¼ˆæˆªæ–­åˆ°ä¸‹ä¸€ä¸ª ``` ä¹‹å‰ï¼‰
                clean_block = block.split('```')[0].strip()
                return json.loads(clean_block)
            except json.JSONDecodeError:
                continue
                
        # 3. å¦‚æœæ²¡æ‰¾åˆ°å®Œæ•´çš„ï¼Œå°è¯•å¤„ç†åªæœ‰å¼€å§‹æ²¡æœ‰ç»“æŸçš„ä»£ç å—ï¼ˆæˆªæ–­æƒ…å†µï¼‰
        # æˆ‘ä»¬åªå…³å¿ƒç¬¬ä¸€ä¸ª ```json å¼€å§‹çš„å†…å®¹
        match = re.search(r'```json\s*(.*)', content, re.DOTALL)
        if match:
            # æˆªå–åˆ°ä¸‹ä¸€ä¸ª ``` å‡ºç°ä¹‹å‰ï¼ˆå¦‚æœæœ‰çš„è¯ï¼Œå¯èƒ½æ˜¯é‡å¤è¾“å‡ºçš„å¼€å§‹ï¼‰
            raw_block = match.group(1)
            # æŸ¥æ‰¾æ˜¯å¦è¿˜æœ‰ä¸‹ä¸€ä¸ª ``` (è¯´æ˜æœ‰é‡å¤è¾“å‡º)
            next_marker = raw_block.find('```')
            if next_marker != -1:
                raw_block = raw_block[:next_marker]
            
            clean_block = raw_block.strip()
            try:
                return json.loads(clean_block)
            except json.JSONDecodeError as e:
                # å°è¯•ä¿®å¤ç”±äºæˆªæ–­å¯¼è‡´çš„ JSON ä¸å®Œæ•´ (åªé’ˆå¯¹æ²¡æœ‰ä¸‹ä¸€ä¸ªæ ‡è®°çš„æƒ…å†µï¼Œå¦‚æœæœ‰ä¸‹ä¸€ä¸ªæ ‡è®°é€šå¸¸æ„å‘³ç€ç¬¬ä¸€å—æ˜¯å®Œæ•´çš„)
                # ä½†å¦‚æœæ˜¯é‡å¤è¾“å‡ºï¼Œraw_block å·²ç»è¢«æˆªæ–­åˆ°å®Œæ•´çš„ç¬¬ä¸€éƒ¨åˆ†äº†ï¼Œåº”è¯¥èƒ½è§£æã€‚
                # å¦‚æœè¿˜æ˜¯è§£æä¸äº†ï¼Œè¯´æ˜ç¬¬ä¸€éƒ¨åˆ†æœ¬èº«ä¹Ÿè¢«æˆªæ–­äº†ï¼Œæˆ–è€…æ ¼å¼é”™è¯¯ã€‚
                pass

        # 4. å°è¯•å¯»æ‰¾ç¬¬ä¸€ä¸ª { å’Œä¸ä¹‹åŒ¹é…çš„ } (ä½¿ç”¨ç®€å•çš„è®¡æ•°å™¨æˆ–æ­£åˆ™)
        # è¿™ç§æ–¹å¼å¯¹ä»˜æ²¡æœ‰ markdown æ ‡è®°çš„è¾“å‡ºå¾ˆæœ‰æ•ˆ
        try:
            start = content.find('{')
            if start != -1:
                # å¯»æ‰¾åŒ¹é…çš„é—­åˆæ‹¬å·
                balance = 0
                for i in range(start, len(content)):
                    if content[i] == '{':
                        balance += 1
                    elif content[i] == '}':
                        balance -= 1
                        if balance == 0:
                            # æ‰¾åˆ°å®Œæ•´é—­åˆçš„ JSON å¯¹è±¡
                            possible_json = content[start:i+1]
                            return json.loads(possible_json)
        except:
            pass
            
        return None


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """è·å–å…¨å±€ LLM å®¢æˆ·ç«¯å®ä¾‹"""
    global _client
    if _client is None:
        _client = LLMClient()
    return _client


def test_llm_connection() -> bool:
    """æµ‹è¯• LLM è¿æ¥"""
    client = get_llm_client()
    if not client.is_available():
        print("âŒ LLM æœåŠ¡ä¸å¯ç”¨")
        return False
    
    print("âœ… LLM æœåŠ¡è¿æ¥æˆåŠŸ")
    response, meta = client.chat([{"role": "user", "content": "å›å¤'OK'"}])
    if response:
        print(f"   æµ‹è¯•å“åº”: {response[:50]}...")
        print(f"   Token ä½¿ç”¨: {meta.get('total_tokens', 'N/A')}")
        return True
    return False


if __name__ == "__main__":
    test_llm_connection()
