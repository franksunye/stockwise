"""
LLM å®¢æˆ·ç«¯æ¨¡å—
å°è£…æœ¬åœ° LLM ä»£ç†æœåŠ¡çš„è°ƒç”¨é€»è¾‘
"""

import json
import requests
from typing import Optional, Dict, Any, Tuple
import time
from config import LLM_CONFIG
from .llm_tracker import get_tracker, estimate_tokens


class LLMClient:
    """æœ¬åœ° LLM ä»£ç†å®¢æˆ·ç«¯"""
    
    def __init__(
        self,
        base_url: str = None,
        api_key: str = None,
        model: str = None,
        timeout: int = 120
    ):
        """
        åˆå§‹åŒ– LLM å®¢æˆ·ç«¯
        
        Args:
            base_url: API åŸºç¡€åœ°å€ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®
            api_key: API å¯†é’¥ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®
            model: æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.base_url = base_url or LLM_CONFIG.get("base_url", "http://127.0.0.1:8045/v1")
        self.api_key = api_key or LLM_CONFIG.get("api_key", "")
        self.model = model or LLM_CONFIG.get("model", "gpt-3.5-turbo")
        self.timeout = timeout
        
    def is_available(self) -> bool:
        """æ£€æŸ¥ LLM æœåŠ¡æ˜¯å¦å¯ç”¨"""
        try:
            response = requests.get(
                f"{self.base_url}/models",
                timeout=5,
                headers={"Authorization": f"Bearer {self.api_key}"}
            )
            return response.status_code == 200
        except:
            return False
    
    def chat(
        self,
        messages: list,
        model: str = None,
        temperature: float = 0.7,
        max_tokens: int = 4096
    ) -> Tuple[Optional[str], Dict[str, Any]]:
        """
        å‘é€èŠå¤©è¯·æ±‚
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨ï¼Œæ ¼å¼ [{"role": "user/system/assistant", "content": "..."}]
            model: ä½¿ç”¨çš„æ¨¡å‹ï¼ˆå¯è¦†ç›–é»˜è®¤ï¼‰
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§è¾“å‡º token æ•°
            
        Returns:
            Tuple: (LLM è¿”å›çš„æ–‡æœ¬å†…å®¹, å…ƒæ•°æ® dict)
            å…ƒæ•°æ®åŒ…å«: input_tokens, output_tokens, total_tokens, latency_ms, error
        """
        payload = {
            "model": model or self.model,
            "messages": messages,
            "temperature": temperature,
            "max_tokens": 4000, # ç¡®ä¿æœ‰è¶³å¤Ÿçš„é•¿åº¦ç”Ÿæˆå®Œæ•´çš„ JSON
        }
        
        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {self.api_key}"
        }
        
        meta = {
            "input_tokens": 0,
            "output_tokens": 0,
            "total_tokens": 0,
            "latency_ms": 0,
            "error": None
        }
        
        try:
            start_time = time.time()
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=self.timeout
            )
            elapsed = time.time() - start_time
            meta["latency_ms"] = int(elapsed * 1000)
            
            if response.status_code == 200:
                data = response.json()
                
                # æå– Token ä½¿ç”¨é‡ (å¦‚æœ API è¿”å›)
                usage = data.get('usage', {})
                if usage:
                    meta["input_tokens"] = usage.get('prompt_tokens', 0)
                    meta["output_tokens"] = usage.get('completion_tokens', 0)
                    meta["total_tokens"] = usage.get('total_tokens', 0)
                
                if data.get('choices'):
                    content = data['choices'][0].get('message', {}).get('content')
                    
                    # å¦‚æœ API æ²¡æœ‰è¿”å› token æ•°ï¼Œä½¿ç”¨ä¼°ç®—
                    if not meta["input_tokens"]:
                        input_text = " ".join([m.get('content', '') for m in messages])
                        meta["input_tokens"] = estimate_tokens(input_text)
                    if not meta["output_tokens"] and content:
                        meta["output_tokens"] = estimate_tokens(content)
                    if not meta["total_tokens"]:
                        meta["total_tokens"] = meta["input_tokens"] + meta["output_tokens"]
                    
                    print(f"   ğŸ¤– LLM å“åº”æˆåŠŸ ({elapsed:.1f}s, {meta['total_tokens']} tokens)")
                    return content, meta
                else:
                    meta["error"] = f"å“åº”æ ¼å¼å¼‚å¸¸: {data}"
                    print(f"   âš ï¸ LLM å“åº”æ ¼å¼å¼‚å¸¸: {data}")
                    return None, meta
            else:
                meta["error"] = f"HTTP {response.status_code}"
                print(f"   âŒ LLM è¯·æ±‚å¤±è´¥: HTTP {response.status_code}")
                return None, meta
                
        except requests.exceptions.Timeout:
            meta["error"] = f"è¯·æ±‚è¶…æ—¶ ({self.timeout}s)"
            print(f"   âŒ LLM è¯·æ±‚è¶…æ—¶ ({self.timeout}s)")
            return None, meta
        except requests.exceptions.ConnectionError:
            meta["error"] = f"æ— æ³•è¿æ¥ LLM æœåŠ¡: {self.base_url}"
            print(f"   âŒ æ— æ³•è¿æ¥ LLM æœåŠ¡: {self.base_url}")
            return None, meta
        except Exception as e:
            meta["error"] = str(e)
            print(f"   âŒ LLM è¯·æ±‚å¼‚å¸¸: {e}")
            return None, meta
    
    def generate_stock_prediction(
        self,
        system_prompt: str,
        user_prompt: str,
        symbol: str = None,
        retries: int = 2
    ) -> Optional[Dict[str, Any]]:
        """
        ç”Ÿæˆè‚¡ç¥¨é¢„æµ‹ï¼ˆå¸¦ JSON è§£æã€é‡è¯•å’Œè¿½è¸ªï¼‰
        
        Args:
            system_prompt: ç³»ç»Ÿæç¤ºè¯ï¼ˆå®šä¹‰è¾“å‡ºæ ¼å¼ï¼‰
            user_prompt: ç”¨æˆ·è¾“å…¥ï¼ˆè‚¡ç¥¨æ•°æ®ï¼‰
            symbol: è‚¡ç¥¨ä»£ç ï¼ˆç”¨äºè¿½è¸ªï¼‰
            retries: é‡è¯•æ¬¡æ•°
            
        Returns:
            è§£æåçš„é¢„æµ‹ç»“æœ dictï¼Œå¤±è´¥è¿”å› None
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ]
        
        # å¼€å§‹è¿½è¸ª
        tracker = get_tracker()
        tracker.start_trace(symbol=symbol, model=self.model)
        tracker.set_prompts(system_prompt, user_prompt)
        
        final_content = None
        final_result = None
        last_meta = {}
        
        for attempt in range(retries + 1):
            if attempt > 0:
                print(f"   ğŸ”„ é‡è¯• {attempt}/{retries}...")
                tracker.increment_retry()
                
            content, meta = self.chat(messages, temperature=0.5)
            last_meta = meta
            
            if content:
                final_content = content
                # å°è¯•è§£æ JSON
                result = self._parse_json_response(content)
                if result:
                    final_result = result
                    break
                else:
                    print(f"   âš ï¸ JSON è§£æå¤±è´¥ï¼ŒåŸå§‹å†…å®¹:\n{content[:500]}...")
        
        # è®°å½•è¿½è¸ªç»“æœ
        tracker.set_tokens(
            input_tokens=last_meta.get("input_tokens", 0),
            output_tokens=last_meta.get("output_tokens", 0),
            total_tokens=last_meta.get("total_tokens", 0)
        )
        tracker.set_response(final_content, final_result)
        
        if final_result:
            tracker.set_status("success")
        elif final_content:
            tracker.set_status("parse_failed", "JSON è§£æå¤±è´¥")
        else:
            tracker.set_status("error", last_meta.get("error", "æœªçŸ¥é”™è¯¯"))
        
        # ç»“æŸè¿½è¸ªå¹¶ä¿å­˜
        trace = tracker.end_trace()
        if trace:
            status_emoji = "âœ…" if trace.status == "success" else "âŒ"
            print(f"   ğŸ“Š è¿½è¸ªå®Œæˆ: {status_emoji} {trace.latency_ms}ms | {trace.total_tokens} tokens | é‡è¯• {trace.retry_count} æ¬¡")
        
        return final_result
    
    def _parse_json_response(self, content: str) -> Optional[Dict[str, Any]]:
        """
        è§£æ LLM è¿”å›çš„ JSON å†…å®¹ï¼ˆæ·±åº¦æ¸…æ´—ç‰ˆï¼‰
        é’ˆå¯¹: Markdown å—ã€éæ ‡å‡†å¼•å·ã€å¼€å¤´ç»“å°¾ä¹±ç ã€è‡ªåŠ¨æˆªæ–­ä¿®å¤
        """
        if not content:
            return None
        
        # 1. å°è¯•æ ‡å‡†è§£æ
        try:
            return json.loads(content)
        except:
            pass
            
        import re
        
        # 2. ç§»é™¤å¸¸è§çš„ Markdown æ ‡è®°
        content_clean = re.sub(r'^```json\s*', '', content, flags=re.MULTILINE)
        content_clean = re.sub(r'^```\s*', '', content_clean, flags=re.MULTILINE)
        content_clean = re.sub(r'```$', '', content_clean, flags=re.MULTILINE)
        try:
            return json.loads(content_clean)
        except:
            pass
            
        # 3. æš´åŠ›æå–æœ€å¤–å±‚çš„ {}
        try:
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ª {
            start_idx = content.find('{')
            if start_idx != -1:
                # å€’åºæ‰¾åˆ°æœ€åä¸€ä¸ª }
                end_idx = content.rfind('}')
                if end_idx != -1 and end_idx > start_idx:
                    possible_json = content[start_idx : end_idx + 1]
                    # å°è¯•æ¸…ç†å¯èƒ½æ··å…¥çš„æ¢è¡Œç¬¦é—®é¢˜
                    possible_json = re.sub(r',\s*}', '}', possible_json) # ç§»é™¤å°¾éšé€—å·
                    return json.loads(possible_json)
        except:
            pass

        # 4. å¦‚æœè¿˜æ˜¯ä¸è¡Œï¼Œå°è¯•ä½¿ç”¨æ ˆå¹³è¡¡æ³•æ‰¾åˆ°å®Œæ•´çš„å¯¹è±¡ (é’ˆå¯¹ç²˜åŒ…/æˆªæ–­)
        try:
            balance = 0
            start = content.find('{')
            if start != -1:
                for i in range(start, len(content)):
                    char = content[i]
                    if char == '{':
                        balance += 1
                    elif char == '}':
                        balance -= 1
                        if balance == 0:
                            # æ‰¾åˆ°äº†ä¸€ä¸ªå®Œæ•´çš„é¡¶å±‚å¯¹è±¡
                            return json.loads(content[start:i+1])
        except:
            pass
            
        return None


# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰
_client: Optional[LLMClient] = None


def get_llm_client() -> LLMClient:
    """è·å–å…¨å±€ LLM å®¢æˆ·ç«¯å®ä¾‹"""
    global _client
    if _client is None:
        _client = LLMClient()
    return _client


def test_llm_connection() -> bool:
    """æµ‹è¯• LLM è¿æ¥"""
    client = get_llm_client()
    if not client.is_available():
        print("âŒ LLM æœåŠ¡ä¸å¯ç”¨")
        return False
    
    print("âœ… LLM æœåŠ¡è¿æ¥æˆåŠŸ")
    response, meta = client.chat([{"role": "user", "content": "å›å¤'OK'"}])
    if response:
        print(f"   æµ‹è¯•å“åº”: {response[:50]}...")
        print(f"   Token ä½¿ç”¨: {meta.get('total_tokens', 'N/A')}")
        return True
    return False


if __name__ == "__main__":
    test_llm_connection()
