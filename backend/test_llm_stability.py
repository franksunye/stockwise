"""
LLM ç¨³å®šæ€§ä¸ JSON è§£æå‹åŠ›æµ‹è¯•è„šæœ¬
ç”¨äºéªŒè¯ Prompt ç»“æ„å’Œå®¢æˆ·ç«¯è§£æå™¨çš„é²æ£’æ€§
"""
import sys
import time
import pandas as pd
from engine.llm_client import get_llm_client
from engine.prompts import prepare_stock_analysis_prompt
from database import get_connection

def get_test_symbol():
    """è·å–ä¸€ä¸ªæœ‰æ•°æ®çš„è‚¡ç¥¨ä»£ç ç”¨äºæµ‹è¯•"""
    conn = get_connection()
    try:
        # è·å–æœ€è¿‘æœ‰è¡Œæƒ…çš„è‚¡ç¥¨
        df = pd.read_sql("SELECT symbol FROM daily_prices ORDER BY date DESC LIMIT 10", conn)
        if not df.empty:
            return df.iloc[0]['symbol']
            
        # å…œåº•
        return "00700" 
    finally:
        conn.close()

def run_stability_test(rounds=5):
    print(f"ğŸš€ å¼€å§‹ LLM ç¨³å®šæ€§æµ‹è¯• (å…± {rounds} è½®)...")
    
    # 1. å‡†å¤‡æ•°æ®
    symbol = get_test_symbol()
    print(f"ğŸ“‹ æµ‹è¯•æ ‡çš„: {symbol}")
    
    try:
        system_prompt, user_prompt = prepare_stock_analysis_prompt(symbol)
    except Exception as e:
        print(f"âŒ å‡†å¤‡ Prompt å¤±è´¥: {e}")
        return

    client = get_llm_client()
    if not client.is_available():
        print("âŒ LLM æœåŠ¡æœªè¿æ¥ï¼Œè¯·æ£€æŸ¥æœ¬åœ°ä»£ç†ã€‚")
        return

    print(f"ğŸ”¹ System Prompt é•¿åº¦: {len(system_prompt)} chars")
    print(f"ğŸ”¹ User Prompt é•¿åº¦: {len(user_prompt)} chars")
    print("-" * 50)

    success_count = 0
    total_latency = 0
    
    for i in range(1, rounds + 1):
        print(f"\nğŸ”„ ç¬¬ {i}/{rounds} è½®æµ‹è¯•ä¸­...", end="", flush=True)
        
        start_time = time.time()
        # å¼ºåˆ¶ä¸é‡è¯• (retries=0)ï¼Œæˆ‘ä»¬è¦çœ‹ä¸€æ¬¡æ€§æˆåŠŸç‡
        # åŒæ—¶ä¸ºäº†æµ‹è¯•ç¨³å®šæ€§ï¼Œè¿™é‡Œæˆ‘ä»¬ç›´æ¥è°ƒç”¨åº•å±‚çš„ chat å’Œ _parse_json_response
        # ä»¥ä¾¿æ•è·åŸå§‹é”™è¯¯ï¼Œè€Œä¸æ˜¯è¢« generate_stock_prediction æ©ç›–
        
        # æ¨¡æ‹Ÿ generate_stock_prediction çš„è¡Œä¸º(å¸¦è¿½è¸ª)
        result = client.generate_stock_prediction(system_prompt, user_prompt, symbol=f"TEST-{i}", retries=0)
        
        duration = time.time() - start_time
        total_latency += duration
        
        if result:
            success_count += 1
            signal = result.get('signal', 'Unknown')
            conf = result.get('confidence', 0)
            print(f" âœ… æˆåŠŸ ({duration:.1f}s) | ä¿¡å·: {signal} | ç½®ä¿¡åº¦: {conf}")
        else:
            print(f" âŒ å¤±è´¥ ({duration:.1f}s)")
            # å°è¯•è·å–æœ€è¿‘ä¸€æ¬¡çš„ trace çœ‹çœ‹å‘ç”Ÿäº†ä»€ä¹ˆ
            # è¿™é‡Œæˆ‘ä»¬ä¸ç›´æ¥è¯»åº“ï¼Œä¸ºäº†ç®€å•ï¼Œæˆ‘ä»¬ä¿¡ä»» client å†…éƒ¨çš„ print è¾“å‡º

    print("-" * 50)
    print(f"ğŸ“Š æµ‹è¯•æ€»ç»“:")
    print(f"   æˆåŠŸç‡: {success_count}/{rounds} ({success_count/rounds*100:.0f}%)")
    print(f"   å¹³å‡è€—æ—¶: {total_latency/rounds:.1f}s")
    
    if success_count < rounds:
        print("\nâš ï¸ å»ºè®®: å¦‚æœå‡ºç°è§£æå¤±è´¥ï¼Œè¯·æ£€æŸ¥ backend/engine/llm_client.py ä¸­çš„ _parse_json_response æ–¹æ³•ï¼Œ"
              "æˆ–è€…æ£€æŸ¥æœ¬åœ° LLM ä»£ç†çš„ stream buffer è®¾ç½®ã€‚")
              
    # å¼ºåˆ¶é€€å‡ºï¼Œé˜²æ­¢ libsql-client åå°çº¿ç¨‹å¡ä½è¿›ç¨‹
    import sys
    sys.exit(0)

if __name__ == "__main__":
    # å¦‚æœå¸¦äº†å‚æ•°ï¼Œä½œä¸ºè½®æ•°
    import sys
    rounds = 3
    if len(sys.argv) > 1:
        try:
            rounds = int(sys.argv[1])
        except:
            pass
            
    run_stability_test(rounds)
