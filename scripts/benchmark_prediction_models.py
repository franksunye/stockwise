"""
Prediction AI Benchmark Tool
Áî®‰∫éÂØπÊØî‰∏çÂêå LLM Ê®°ÂûãÁîüÊàêÁªìÊûÑÂåñÊäïËµÑÂª∫ËÆÆÁöÑËÉΩÂäõÔºàSignal/Confidence/Reasoning/TacticsÔºâ„ÄÇ
ÊµãËØïÊï∞ÊçÆÊù•Ëá™ÁúüÂÆûÁöÑ LLM Trace„ÄÇ

‰ΩøÁî®ÊñπÊ≥ï:
  python scripts/benchmark_prediction_models.py                    # ËøêË°åÊµãËØï
  python scripts/benchmark_prediction_models.py --model hunyuan-turbo
  python scripts/benchmark_prediction_models.py --fetch           # Âº∫Âà∂ÈáçÊñ∞‰ªéÊï∞ÊçÆÂ∫ìÊãâÂèñÊµãËØïÁî®‰æã
"""

import os
import sys
import time
import json
import argparse
import asyncio
from pathlib import Path
from typing import Dict, Any, List, Tuple

# Add backend to path
root_dir = Path(__file__).parent.parent
sys.path.append(str(root_dir))
sys.path.append(str(root_dir / "backend"))

from dotenv import load_dotenv
load_dotenv(root_dir / "backend" / ".env")

# =====================================================
# Ê®°ÂûãÈÖçÁΩÆ
# =====================================================
MODEL_CONFIGS = {
    # Gemini Local (Âü∫ÂáÜ)
    "gemini-local": {
        "provider": "openai",  # OpenAI Protocol
        "base_url": os.getenv("GEMINI_LOCAL_BASE_URL", "http://127.0.0.1:8045") + "/v1",
        "api_key": os.getenv("LLM_API_KEY", "sk-test"),
        "model": os.getenv("GEMINI_LOCAL_MODEL", "gemini-3-flash"),
        "description": "Êú¨Âú∞ Gemini ‰ª£ÁêÜ (Âü∫ÂáÜ)",
    },
    # Ê∑∑ÂÖÉÊ®°Âûã
    "hunyuan-lite": {
        "provider": "openai",
        "base_url": "https://api.hunyuan.cloud.tencent.com/v1",
        "api_key": os.getenv("HUNYUAN_API_KEY"),
        "model": "hunyuan-lite",
        "description": "ËÖæËÆØÊ∑∑ÂÖÉ Lite (ÂÖçË¥π)",
    },
    "hunyuan-turbo": {
        "provider": "openai",
        "base_url": "https://api.hunyuan.cloud.tencent.com/v1",
        "api_key": os.getenv("HUNYUAN_API_KEY"),
        "model": "hunyuan-turbo",
        "description": "ËÖæËÆØÊ∑∑ÂÖÉ Turbo (Êé®Ëçê)",
    },
    "hunyuan-pro": {
        "provider": "openai",
        "base_url": "https://api.hunyuan.cloud.tencent.com/v1",
        "api_key": os.getenv("HUNYUAN_API_KEY"),
        "model": "hunyuan-pro",
        "description": "ËÖæËÆØÊ∑∑ÂÖÉ Pro (ÈÄªËæëÂº∫)",
    },
}

# =====================================================
# Á≥ªÁªüÊèêÁ§∫ËØç (Á≤æÁÆÄÁâàÔºåÁî®‰∫é Mock)
# =====================================================
# Ê≥®ÊÑèÔºöÂÆûÈôÖËØ∑Ê±Ç‰∏≠ÔºåÊàë‰ª¨‰ΩøÁî® Trace ‰∏≠ÁöÑ user_promptÔºå‰ΩÜÊàë‰ª¨‰πüÈúÄË¶Å‰∏Ä‰∏™ System Prompt„ÄÇ
# ËøôÈáåÊàë‰ª¨Á°¨ÁºñÁ†Å backend/engine/prompts.py ‰∏≠ÁöÑ System PromptÔºåÁ°Æ‰øùÁéØÂ¢É‰∏ÄËá¥„ÄÇ
SYSTEM_PROMPT = """‰Ω†ÊòØ StockWise ÁöÑ AI ÂÜ≥Á≠ñÂä©ÊâãÔºå‰∏ìÈó®‰∏∫‰∏™‰∫∫ÊäïËµÑËÄÖÊèê‰æõËÇ°Á•®Êìç‰ΩúÂª∫ËÆÆ„ÄÇ

## ‰Ω†ÁöÑÊ†∏ÂøÉÂéüÂàôÔºö
1. **ÁêÜÊÄßÈîöÁÇπ**Ôºö‰Ω†‰∏çÈ¢ÑÊµãÊ∂®Ë∑åÔºå‰Ω†Êèê‰æõ"ÊâßË°åÁ∫™Âæã"ÁöÑËß¶ÂèëÊù°‰ª∂„ÄÇ
2. **‰∏™ÊÄßÂåñ**ÔºöÊ†πÊçÆÁî®Êà∑ÊòØ"Â∑≤ÊåÅ‰ªì"ËøòÊòØ"Êú™Âª∫‰ªì"ÔºåÊèê‰æõÂ∑ÆÂºÇÂåñÁöÑË°åÂä®Âª∫ËÆÆ„ÄÇ
3. **ÂèØÈ™åËØÅ**ÔºöÊØèÊù°Âª∫ËÆÆÈÉΩÊúâÊòéÁ°ÆÁöÑËß¶ÂèëÊù°‰ª∂„ÄÇ
4. **ÁÆÄÊ¥ÅÁõ¥ÁôΩ**Ôºö‰ΩøÁî®ÊôÆÈÄö‰∫∫ËÉΩÁßíÊáÇÁöÑËØ≠Ë®Ä„ÄÇ

## ‰Ω†ÁöÑËæìÂá∫Ê†ºÂºèÔºö
‰Ω†ÂøÖÈ°ª‰∏•Ê†ºÊåâÁÖß‰ª•‰∏ã JSON Ê†ºÂºèËæìÂá∫Ôºå‰∏çË¶ÅÊ∑ªÂä†‰ªª‰ΩïÂÖ∂‰ªñÊñáÂ≠óÔºö

{
  "signal": "Long" | "Side" | "Short",
  "confidence": 0.0 ~ 1.0,
  "summary": "‰∏ÄÂè•ËØùÊ†∏ÂøÉÁªìËÆ∫Ôºà15Â≠ó‰ª•ÂÜÖÔºâ",
  "reasoning_trace": [
    { "step": "trend", "data": "Ë∂ãÂäøÊï∞ÊçÆ", "conclusion": "ÁªìËÆ∫" },
    { "step": "momentum", "data": "Âä®ËÉΩÊï∞ÊçÆ", "conclusion": "ÁªìËÆ∫" },
    { "step": "decision", "data": "ÁªºÂêàÂõ†Á¥†", "conclusion": "ÂÜ≥Á≠ñ" }
  ],
  "tactics": {
    "holding": [{"priority": "P1", "action": "...", "trigger": "...", "reason": "..."}],
    "empty": [],
    "general": []
  },
  "key_levels": { "support": 0, "resistance": 0, "stop_loss": 0 },
  "conflict_resolution": "...",
  "tomorrow_focus": "..."
}"""

# =====================================================
# Êï∞ÊçÆËé∑Âèñ (Mock or Turso)
# =====================================================
CACHE_FILE = Path(__file__).parent / "temp_prediction_case.json"

async def fetch_real_trace_case():
    """‰ªéÊï∞ÊçÆÂ∫ìËé∑ÂèñÁúüÂÆûÁöÑ Prompt"""
    from database import get_connection
    print("üì° ËøûÊé•Êï∞ÊçÆÂ∫ìËé∑ÂèñÁúüÂÆû User Prompt...")
    
    conn = get_connection()
    try:
        cursor = conn.cursor()
        # Ëé∑ÂèñÊúÄËøë‰∏ÄÊù°ÊàêÂäüÁöÑ JSON Ê†ºÂºèÁöÑ trace (ÈÄöÂ∏∏ tokens ÊØîËæÉÂ§ö)
        cursor.execute("""
            SELECT symbol, user_prompt 
            FROM llm_traces 
            WHERE status = 'success' 
            AND length(user_prompt) > 500
            ORDER BY created_at DESC 
            LIMIT 1
        """)
        row = cursor.fetchone()
        
        if not row:
            print("‚ùå Êú™ÊâæÂà∞ÂêàÈÄÇÁöÑ Trace ËÆ∞ÂΩï„ÄÇ")
            return None
            
        case = {
            "symbol": row[0],
            "user_prompt": row[1]
        }
        
        # Save to cache
        with open(CACHE_FILE, "w", encoding="utf-8") as f:
            json.dump(case, f, ensure_ascii=False, indent=2)
            
        print(f"‚úÖ Ëé∑ÂèñÊàêÂäü: {case['symbol']} (Prompt Length: {len(case['user_prompt'])})")
        return case
        
    except Exception as e:
        print(f"‚ùå Êï∞ÊçÆÂ∫ìÈîôËØØ: {e}")
        return None
    finally:
        conn.close()

def get_test_case(force_fetch=False):
    if not force_fetch and CACHE_FILE.exists():
        print(f"üìÇ ‰ΩøÁî®ÁºìÂ≠òÁöÑÊµãËØïÁî®‰æã: {CACHE_FILE}")
        with open(CACHE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    
    # Â∞ùËØïÂºÇÊ≠•Ëé∑Âèñ
    try:
        result = asyncio.run(fetch_real_trace_case())
        if result: return result
    except ImportError:
        print("‚ö†Ô∏è Êó†Ê≥ïÂØºÂÖ• database Ê®°Âùó (ÂèØËÉΩÊòØË∑ØÂæÑÈóÆÈ¢ò)Ôºå‰ΩøÁî® Mock Êï∞ÊçÆ„ÄÇ")
    except Exception as e:
        print(f"‚ö†Ô∏è Ëé∑ÂèñÊï∞ÊçÆÂ§±Ë¥•: {e}")
    
    # Fallback Mock Data
    print("‚ö†Ô∏è ‰ΩøÁî®ÂÜÖÁΩÆ Mock Êï∞ÊçÆ")
    return {
        "symbol": "MOCK",
        "user_prompt": "Subject: 00700 (ËÖæËÆØÊéßËÇ°)\n\n[Daily Data]\nDate: 2026-01-08\nClose: 398.00\nRSI: 45\nMACD: 0.5\n\n[History]\nTrend: Bearish 5 days\n\nInstruction: Provide trading advice."
    }

# =====================================================
# LLM Ë∞ÉÁî®
# =====================================================
def call_model(config: Dict, user_prompt: str) -> Tuple[Dict, float, Dict]:
    from openai import OpenAI
    
    if not config.get("api_key"):
        return {}, 0, {"error": "Missing API Key"}
    
    client = OpenAI(
        api_key=config["api_key"],
        base_url=config["base_url"],
    )
    
    start_time = time.time()
    try:
        response = client.chat.completions.create(
            model=config["model"],
            messages=[
                {"role": "system", "content": SYSTEM_PROMPT},
                {"role": "user", "content": user_prompt},
            ],
            temperature=0.1, # È¢ÑÊµã‰ªªÂä°ÈúÄË¶Å‰ΩéÊ∏©Â∫¶
            response_format={"type": "json_object"}, # Âº∫Âà∂ JSON
            max_tokens=2000,
        )
        elapsed = time.time() - start_time
        
        raw_content = response.choices[0].message.content
        usage = response.usage
        
        meta = {
            "input_tokens": usage.prompt_tokens if usage else 0,
            "output_tokens": usage.completion_tokens if usage else 0,
            "total_tokens": usage.total_tokens if usage else 0,
        }
        
        # Parse JSON
        try:
            parsed = json.loads(raw_content)
            return parsed, elapsed, meta
        except json.JSONDecodeError:
            return {"error": "Invalid JSON", "raw": raw_content[:200]}, elapsed, meta
            
    except Exception as e:
        elapsed = time.time() - start_time
        return {}, elapsed, {"error": str(e)}

# =====================================================
# ‰∏ªÁ®ãÂ∫è
# =====================================================
def run_benchmark(model_filter: str = None, force_fetch: bool = False):
    # Auto-disable proxy for Hunyuan
    os.environ["NO_PROXY"] = "api.hunyuan.cloud.tencent.com"
    
    case = get_test_case(force_fetch)
    if not case: return
    
    models_to_test = MODEL_CONFIGS.keys() if not model_filter else [model_filter]
    
    print("=" * 60)
    print("üß† Prediction AI Benchmark (JSON Logic)")
    print(f"Target Symbol: {case['symbol']}")
    print("=" * 60)
    
    results = []
    
    for model_id in models_to_test:
        if model_id not in MODEL_CONFIGS: continue
        config = MODEL_CONFIGS[model_id]
        
        print(f"\nü§ñ {model_id}...", end=" ", flush=True)
        
        parsed, elapsed, meta = call_model(config, case['user_prompt'])
        
        if "error" in meta:
            print(f"‚ùå Error: {meta['error']}")
        elif "error" in parsed:
            print(f"‚ùå JSON Parse Error: {parsed['raw']}...")
        else:
            print(f"‚úÖ ({elapsed:.1f}s)")
            
            # Ë¥®ÈáèÊ£ÄÊü•
            signal = parsed.get("signal", "N/A")
            conf = parsed.get("confidence", 0)
            reasoning_len = len(str(parsed.get("reasoning_trace", "")))
            tactics_count = len(parsed.get("tactics", {}).get("holding", []))
            
            print(f"   üìä Signal: {signal:<6} | Confidence: {conf:.2f}")
            print(f"   üí° Summary: {parsed.get('summary', 'N/A')}")
            print(f"   ‚õìÔ∏è Trace Points: {len(parsed.get('reasoning_trace', []))} steps")
            print(f"   üõ°Ô∏è Holding Tactics: {tactics_count}")
        
        results.append({
            "model": model_id,
            "elapsed": elapsed,
            "success": "error" not in meta and "error" not in parsed,
            "parsed": parsed
        })

    # Summary
    print("\n" + "=" * 60)
    print("üìà ÁªìÊûúÊ±áÊÄª")
    for r in results:
        status = "‚úÖ" if r['success'] else "‚ùå"
        print(f"{status} {r['model']:15} | {r['elapsed']:.1f}s")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str)
    parser.add_argument("--fetch", action="store_true", help="Âº∫Âà∂‰ªé DB ÊãâÂèñÊñ∞ Case")
    args = parser.parse_args()
    
    run_benchmark(args.model, args.fetch)
